defaults:
  - input_encoder: triplet_encoder
  - backbone: triplet_transformer_encoder
  - text_encoder: bert_encoder
  - _self_
_target_: meds_torch.models.bicl.BICLModule.initialize

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.001
  weight_decay: 0.01

scheduler:
  _target_: meds_torch.utils.custom_scheduler.CosineAnnealingWithLinearWarmup
  _partial_: true
  warmup_iters: min

_resolved_max_seq_len: ${data._resolved_max_seq_len}
token_dim: 4
vocab_size: ${data.vocab_size}
get_representations: false
task_names: [
  "${data.eval_datasets[1].task_name}", 
  "${data.eval_datasets[2].task_name}",
]
batch_size: ${data.dataloader.batch_size}
tau: 0.07
tokenizer: "emilyalsentzer/Bio_ClinicalBERT"
zeroshot_templates: ["patient deceased", "discharged today"]

# compile model for faster training with pytorch 2.0
compile: false
