_target_: meds_torch.input_encoder.text_encoder.TextCodeEncoder.initialize
collate_style: text_code
_resolved_max_seq_len: ${data._resolved_max_seq_len}
token_dim: ${model.token_dim}
vocab_size: ${data.vocab_size}

text_max_seq_len: ${data.text_max_seq_len}
dropout: ${model.backbone.dropout}
text_token_dim: 4
text_n_layers: 4
text_nheads: 4

auto_embedder:
  _target_: meds_torch.input_encoder.text_encoder.AutoEmbedder
  code_embedder:
    _target_: x_transformers.TransformerWrapper
    logits_dim: ${model.input_encoder.text_token_dim}
    num_tokens: ${get_text_vocab_size:${data.tokenizer}}
    max_seq_len: ${data.text_max_seq_len}
    emb_dropout: ${model.backbone.dropout}
    use_abs_pos_emb: false
    attn_layers:
      _target_: x_transformers.Encoder
      dim: ${model.input_encoder.text_token_dim}
      depth: ${model.input_encoder.text_n_layers}
      heads: ${model.input_encoder.text_nheads}
      layer_dropout: ${model.backbone.dropout}  # stochastic depth - dropout entire layer
      attn_dropout: ${model.backbone.dropout}  # dropout post-attention
      ff_dropout: ${model.backbone.dropout}  # feedforward dropout
      rotary_pos_emb: true
    use_cls_token: true
