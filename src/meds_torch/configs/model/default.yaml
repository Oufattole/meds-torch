architecture: transformer
get_last_token: true
params:
  nheads: 2
  n_layers: 2
  dropout: 0.1

embedder:
  collate_style: triplet
  max_seq_len: 512
  token_dim: 4
  vocab_size: ${get_vocab_size:${code_metadata_fp}}
